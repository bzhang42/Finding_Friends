{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import agents\n",
    "import game\n",
    "import mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'mechanisms' from 'C:\\\\Users\\\\billz\\\\PycharmProjects\\\\CS_281\\\\Finding_Friends\\\\mechanisms.py'>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(agents)\n",
    "importlib.reload(game)\n",
    "importlib.reload(mechanisms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "memory = ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQ_Learner(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQ_Learner, self).__init__()\n",
    "        self.head = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.head(x)\n",
    "\n",
    "class DQ_Agent(agents.Agent):\n",
    "    def __init__(self, id, level, input_dim, output_dim):\n",
    "        super(DQ_Agent, self).__init__(id, level)\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.learner = DQ_Learner(input_dim, output_dim).to(device)\n",
    "        self.steps = 0\n",
    "        \n",
    "    def pick_friends(self, levels, cap, skill_levels=None):\n",
    "        state = [cap] + levels\n",
    "        if skill_levels is not None:\n",
    "            state = state + skill_levels\n",
    "        \n",
    "        state = torch.tensor(state, device=device, dtype=torch.float)\n",
    "        print(state)\n",
    "        \n",
    "        if self.last_reward is not None:\n",
    "            memory.push(self.last_state, self.last_action, state, self.last_reward)\n",
    "        \n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        \n",
    "        # Perform e-greedy policy \n",
    "        sample = random.random()\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * self.steps / EPS_DECAY)\n",
    "        self.steps += 1\n",
    "        if sample > eps_threshold:\n",
    "            print('Policy')\n",
    "            with torch.no_grad():\n",
    "#                 temp = self.learner(state)\n",
    "#                 print(temp)\n",
    "#                 print(temp.max(0))\n",
    "#                 print(temp.max(0)[1])\n",
    "#                 print(temp.max(0)[1].view(1, 1))\n",
    "                action = self.learner(state).max(0)[1].view(1, 1)\n",
    "        else:\n",
    "            print('Explore')\n",
    "            action = torch.tensor([[np.random.randint(1, len(levels))]], device=device, dtype=torch.long)\n",
    "#         probs = self.learner(state)\n",
    "#         candidates = list(zip([i for i in range(len(levels))], probs))\n",
    "#         candidates = sorted(candidates, key=lambda t: t[1])\n",
    "        \n",
    "#         action = candidates[0][0]\n",
    "        \n",
    "        self.last_action = action\n",
    "        self.last_state = state\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def accept_reward(self, reward, done):\n",
    "        self.last_reward = torch.tensor([reward], device=device)\n",
    "        if done:\n",
    "            memory.push(self.last_state, self.last_action, None, self.last_reward)\n",
    "            \n",
    "            # Perform one step of the optimization (on the target network)\n",
    "            optimize_model()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "NUM_PLAYERS = 5\n",
    "BASE_LVL = 0\n",
    "LVL_CAP = 100\n",
    "WIN_PROB = 0.4\n",
    "\n",
    "ff_mech = mechanisms.Baseline_Mechanism(num_players=NUM_PLAYERS, p=WIN_PROB)\n",
    "\n",
    "input_dim = ff_mech.input_dim()\n",
    "output_dim = ff_mech.output_dim()\n",
    "\n",
    "policy_agent = DQ_Agent(0, BASE_LVL, input_dim, output_dim)\n",
    "target_agent = DQ_Agent(1000, BASE_LVL, input_dim, output_dim)\n",
    "\n",
    "policy_net = policy_agent.learner\n",
    "target_net = target_agent.learner\n",
    "# policy_net = DQ_Learner(input_dim, output_dim).to(device)\n",
    "# target_net = DQ_Learner(input_dim, output_dim).to(device)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "# Don't need this because using the agent instead\n",
    "# def select_action(state):\n",
    "#     global steps_done\n",
    "#     sample = random.random()\n",
    "#     eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "#     steps_done += 1\n",
    "#     if sample > eps_threshold:\n",
    "#         with torch.no_grad():\n",
    "#             return policy_net(state).max(1)[1].view(1, 1)\n",
    "#     else:\n",
    "#         return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "episode_scores = []\n",
    "\n",
    "def plot_scores():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    scores_t = torch.tensor(episode_scores, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(scores_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(scores_t) >= 100:\n",
    "        means = scores_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('head.weight',\n",
       "              tensor([[ 0.1522,  0.3403,  0.3369, -0.1684, -0.2320,  0.0282],\n",
       "                      [ 0.2582,  0.1926,  0.1686, -0.0947, -0.0910, -0.0108],\n",
       "                      [-0.1405,  0.3854, -0.2179, -0.3075, -0.3374,  0.1989],\n",
       "                      [-0.0748,  0.0072,  0.0632, -0.0172, -0.0996, -0.1477],\n",
       "                      [-0.1650,  0.0685, -0.2966,  0.3442,  0.3576, -0.3673]])),\n",
       "             ('head.bias',\n",
       "              tensor([-0.2603, -0.3429,  0.2978, -0.0718,  0.3076]))])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    \n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    print(state_batch)\n",
    "    \n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([100.,   0.,   0.,   1.,   0.,   1.])\n",
      "Explore\n",
      "tensor([100.,   1.,   1.,   2.,   0.,   2.])\n",
      "Explore\n",
      "tensor([100.,   3.,   2.,   3.,   0.,   2.])\n",
      "Explore\n",
      "tensor([100.,   4.,   4.,   3.,   1.,   2.])\n",
      "Explore\n",
      "tensor([100.,   7.,   5.,   4.,   3.,   3.])\n",
      "Policy\n",
      "tensor([100.,   8.,   6.,   4.,   3.,   3.])\n",
      "Explore\n",
      "tensor([100.,   9.,   8.,   4.,   4.,   3.])\n",
      "Policy\n",
      "tensor([100.,   9.,   9.,   5.,   5.,   4.])\n",
      "Explore\n",
      "tensor([100.,  11.,  11.,   6.,   7.,   5.])\n",
      "Explore\n",
      "tensor([100.,  12.,  11.,   6.,   8.,   7.])\n",
      "Explore\n",
      "tensor([100.,  15.,  13.,   7.,   9.,   8.])\n",
      "Explore\n",
      "tensor([100.,  17.,  14.,   7.,   9.,   9.])\n",
      "Explore\n",
      "tensor([100.,  18.,  16.,   7.,  10.,  11.])\n",
      "Policy\n",
      "tensor([100.,  19.,  17.,   7.,  13.,  12.])\n",
      "Explore\n",
      "tensor([100.,  20.,  17.,   7.,  14.,  12.])\n",
      "Explore\n",
      "tensor([100.,  20.,  17.,   7.,  15.,  13.])\n",
      "Explore\n",
      "tensor([100.,  20.,  17.,   7.,  15.,  13.])\n",
      "Policy\n",
      "tensor([100.,  21.,  19.,   8.,  17.,  15.])\n",
      "Explore\n",
      "tensor([100.,  21.,  20.,   8.,  17.,  16.])\n",
      "Explore\n",
      "tensor([100.,  21.,  20.,   8.,  17.,  16.])\n",
      "Policy\n",
      "tensor([100.,  21.,  21.,  10.,  18.,  18.])\n",
      "Explore\n",
      "tensor([100.,  21.,  21.,  11.,  19.,  18.])\n",
      "Explore\n",
      "tensor([100.,  23.,  22.,  14.,  20.,  21.])\n",
      "Explore\n",
      "tensor([100.,  24.,  23.,  16.,  22.,  23.])\n",
      "Explore\n",
      "tensor([100.,  25.,  23.,  17.,  24.,  23.])\n",
      "Policy\n",
      "tensor([100.,  25.,  25.,  18.,  25.,  25.])\n",
      "Policy\n",
      "tensor([100.,  27.,  26.,  19.,  26.,  26.])\n",
      "Explore\n",
      "tensor([100.,  27.,  26.,  19.,  26.,  26.])\n",
      "Explore\n",
      "tensor([100.,  29.,  28.,  21.,  27.,  27.])\n",
      "Explore\n",
      "tensor([100.,  29.,  28.,  23.,  27.,  29.])\n",
      "Explore\n",
      "tensor([100.,  31.,  28.,  24.,  27.,  30.])\n",
      "Policy\n",
      "tensor([100.,  31.,  28.,  26.,  29.,  30.])\n",
      "tensor([100.,  19.,  20.,  21.,  20.,  18., 100.,   2.,   4.,   4.,   5.,   3.,\n",
      "        100.,  72.,  76.,  82.,  62.,  72., 100.,  21.,  20.,   8.,  17.,  16.,\n",
      "        100.,  20.,  17.,   7.,  14.,  12., 100.,  15.,  15.,  15.,  15.,  16.,\n",
      "        100.,   7.,   5.,   4.,   3.,   3., 100.,  44.,  48.,  47.,  43.,  46.,\n",
      "        100.,   1.,   2.,   3.,   4.,   2., 100.,  17.,  19.,  16.,  18.,  18.,\n",
      "        100.,  11.,  10.,  10.,  10.,   9., 100.,  72.,  75.,  81.,  62.,  72.,\n",
      "        100.,   4.,   4.,   3.,   1.,   2., 100.,  65.,  69.,  73.,  56.,  67.,\n",
      "        100.,  61.,  64.,  68.,  54.,  65., 100.,  25.,  23.,  17.,  24.,  23.,\n",
      "        100.,  45.,  49.,  48.,  44.,  48., 100.,  65.,  71.,  74.,  57.,  67.,\n",
      "        100.,  17.,  14.,   7.,   9.,   9., 100.,  37.,  38.,  42.,  39.,  36.,\n",
      "        100.,   5.,   7.,   6.,   5.,   5., 100.,   3.,   2.,   3.,   0.,   2.,\n",
      "        100.,  54.,  55.,  55.,  47.,  55., 100.,  16.,  18.,  16.,  18.,  18.,\n",
      "        100.,  74.,  81.,  86.,  65.,  74., 100.,   9.,   8.,   4.,   4.,   3.,\n",
      "        100.,   1.,   1.,   2.,   0.,   2., 100.,  32.,  35.,  37.,  34.,  30.,\n",
      "        100.,  28.,  29.,  29.,  28.,  24., 100.,  15.,  18.,  16.,  16.,  17.,\n",
      "        100.,  38.,  38.,  42.,  39.,  37., 100.,  34.,  36.,  38.,  35.,  31.,\n",
      "        100.,  42.,  44.,  45.,  43.,  44., 100.,  31.,  28.,  24.,  27.,  30.,\n",
      "        100.,  70.,  74.,  80.,  59.,  71., 100.,  22.,  26.,  26.,  25.,  23.,\n",
      "        100.,  20.,  20.,  23.,  21.,  18., 100.,  31.,  33.,  36.,  31.,  29.,\n",
      "        100.,  64.,  68.,  71.,  55.,  66., 100.,  11.,  11.,   6.,   7.,   5.,\n",
      "        100.,  32.,  34.,  37.,  32.,  29., 100.,  11.,  10.,  10.,  10.,   9.,\n",
      "        100.,  80.,  84.,  99.,  75.,  86., 100.,  57.,  59.,  64.,  49.,  61.,\n",
      "        100.,  75.,  81.,  89.,  67.,  76., 100.,  79.,  83.,  98.,  75.,  85.,\n",
      "        100.,  50.,  53.,  52.,  46.,  51., 100.,   0.,   0.,   1.,   0.,   1.,\n",
      "        100.,  55.,  57.,  61.,  49.,  58., 100.,  13.,  13.,  10.,  12.,  12.,\n",
      "        100.,  46.,  50.,  48.,  44.,  48., 100.,  20.,  20.,  23.,  21.,  18.,\n",
      "        100.,  48.,  51.,  50.,  44.,  49., 100.,  76.,  81.,  92.,  69.,  82.,\n",
      "        100.,  27.,  26.,  19.,  26.,  26., 100.,  29.,  28.,  21.,  27.,  27.,\n",
      "        100.,  56.,  58.,  63.,  49.,  60., 100.,  24.,  23.,  16.,  22.,  23.,\n",
      "        100.,  20.,  17.,   7.,  15.,  13., 100.,   9.,   9.,   5.,   5.,   4.,\n",
      "        100.,  20.,  22.,  23.,  22.,  19., 100.,  21.,  20.,   8.,  17.,  16.,\n",
      "        100.,  80.,  84.,  99.,  75.,  86., 100.,  78.,  82.,  95.,  73.,  84.,\n",
      "        100.,  10.,  10.,   9.,   9.,   8., 100.,  41.,  41.,  45.,  42.,  41.,\n",
      "        100.,  73.,  79.,  83.,  64.,  73., 100.,   8.,  10.,   7.,   8.,   7.,\n",
      "        100.,  75.,  81.,  88.,  66.,  74., 100.,  21.,  21.,  11.,  19.,  18.,\n",
      "        100.,  52.,  54.,  53.,  47.,  52., 100.,  18.,  16.,   7.,  10.,  11.,\n",
      "        100.,  21.,  21.,  10.,  18.,  18., 100.,   7.,   9.,   7.,   6.,   7.,\n",
      "        100.,  68.,  71.,  75.,  57.,  69., 100.,  78.,  82.,  95.,  73.,  84.,\n",
      "        100.,  29.,  32.,  34.,  30.,  27., 100.,  13.,  11.,  10.,  12.,  10.,\n",
      "        100.,  46.,  51.,  49.,  44.,  48., 100.,  76.,  81.,  90.,  68.,  79.,\n",
      "        100.,  70.,  73.,  79.,  59.,  71., 100.,  14.,  13.,  11.,  13.,  13.,\n",
      "        100.,  47.,  51.,  49.,  44.,  49., 100.,  22.,  26.,  26.,  24.,  22.,\n",
      "        100.,  22.,  23.,  25.,  23.,  21., 100.,  44.,  48.,  48.,  43.,  47.,\n",
      "        100.,  55.,  56.,  59.,  49.,  57., 100.,  15.,  13.,   7.,   9.,   8.,\n",
      "        100.,  29.,  28.,  23.,  27.,  29., 100.,  20.,  17.,   7.,  15.,  13.,\n",
      "        100.,  62.,  66.,  69.,  55.,  66., 100.,  39.,  40.,  42.,  41.,  38.,\n",
      "        100.,  15.,  17.,  16.,  15.,  17., 100.,  25.,  25.,  18.,  25.,  25.,\n",
      "        100.,  27.,  26.,  19.,  26.,  26., 100.,   1.,   2.,   2.,   3.,   2.,\n",
      "        100.,  23.,  22.,  14.,  20.,  21., 100.,  64.,  67.,  70.,  55.,  66.,\n",
      "        100.,  29.,  32.,  32.,  30.,  25., 100.,  73.,  80.,  85.,  65.,  73.,\n",
      "        100.,  41.,  41.,  45.,  42.,  41., 100.,  77.,  81.,  93.,  70.,  83.,\n",
      "        100.,  27.,  28.,  27.,  28.,  24., 100.,  59.,  61.,  67.,  52.,  63.,\n",
      "        100.,  49.,  51.,  51.,  45.,  50., 100.,  43.,  46.,  45.,  43.,  45.,\n",
      "        100.,  35.,  37.,  40.,  36.,  32., 100.,  41.,  40.,  43.,  41.,  39.,\n",
      "        100.,  12.,  11.,   6.,   8.,   7., 100.,  53.,  54.,  54.,  47.,  54.,\n",
      "        100.,  26.,  28.,  27.,  27.,  24., 100.,  69.,  73.,  77.,  57.,  70.,\n",
      "        100.,   1.,   1.,   1.,   1.,   0., 100.,   0.,   0.,   0.,   0.,   0.,\n",
      "        100.,  19.,  17.,   7.,  13.,  12., 100.,  70.,  73.,  77.,  58.,  70.,\n",
      "        100.,  35.,  38.,  42.,  37.,  34., 100.,  18.,  19.,  18.,  19.,  18.,\n",
      "        100.,  12.,  10.,  10.,  12.,  10., 100.,  52.,  54.,  53.,  47.,  52.,\n",
      "        100.,   4.,   6.,   5.,   5.,   4., 100.,   8.,   6.,   4.,   3.,   3.,\n",
      "        100.,  28.,  30.,  31.,  28.,  25., 100.,  57.,  59.,  66.,  50.,  62.,\n",
      "        100.,  55.,  56.,  57.,  48.,  56., 100.,  14.,  14.,  12.,  14.,  14.,\n",
      "        100.,  49.,  51.,  52.,  46.,  50., 100.,  21.,  19.,   8.,  17.,  15.])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1 x 768], m2: [6 x 5] at c:\\n\\pytorch_1559129895673\\work\\aten\\src\\th\\generic/THTensorMath.cpp:940",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-146-2464c4b016e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mff_game\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mepisode_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mff_game\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mplot_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\CS_281\\Finding_Friends\\game.py\u001b[0m in \u001b[0;36mplay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[1;31m# The mechanism is used to determine the new levels of all players\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             \u001b[0mnew_levels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmechanism\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mking\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[1;31m# The rewards are distributed based on these new levels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\CS_281\\Finding_Friends\\mechanisms.py\u001b[0m in \u001b[0;36mplay\u001b[1;34m(self, player, levels, cap)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;31m# Let the player who is king pick the friend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[0mfriend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpick_friends\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;31m# Sample and increase levels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-142-35933f69b567>\u001b[0m in \u001b[0;36mpick_friends\u001b[1;34m(self, levels, cap, skill_levels)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# Perform one step of the optimization (on the target network)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0moptimize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# Perform e-greedy policy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-145-f7696fb87289>\u001b[0m in \u001b[0;36moptimize_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# columns of actions taken. These are the actions which would've been taken\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# for each batch state according to policy_net\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mstate_action_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# Compute V(s_{t+1}) for all next states.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-142-35933f69b567>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDQ_Agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAgent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1352\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1354\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1355\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1356\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch, m1: [1 x 768], m2: [6 x 5] at c:\\n\\pytorch_1559129895673\\work\\aten\\src\\th\\generic/THTensorMath.cpp:940"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_episodes = 50\n",
    "\n",
    "ff_agents = [policy_agent] + [agents.Basic_Agent(i, BASE_LVL) for i in range(1, NUM_PLAYERS)]\n",
    "ff_game = game.Game(players=ff_agents, mechanism=ff_mech, cap=LVL_CAP, logging_level='DEBUG')\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    ff_game.play()\n",
    "    episode_scores.append(ff_game.levels[0])\n",
    "    plot_scores()\n",
    "    ff_game.reset()\n",
    "    \n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "print('Complete')\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
